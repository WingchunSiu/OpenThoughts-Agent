from __future__ import annotations

import textwrap
from pathlib import Path
from typing import Any, Callable, Optional

import yaml


def apply_mca_training_template(
    exp_args: dict,
    hpc,
    *,
    update_exp_args_fn: Callable[[dict, dict], dict],
) -> dict:
    """Point training jobs at the MCA-specific sbatch template when requested."""

    mca_template = Path(__file__).parent / "sbatch" / f"{hpc.name.lower()}_train_mca.sbatch"
    if mca_template.exists():
        return update_exp_args_fn(
            exp_args,
            {
                "train_sbatch_filename": mca_template.name,
                "train_sbatch_path": str(mca_template),
            },
        )

    print(
        f"Warning: MCA sbatch template {mca_template} not found for cluster {hpc.name}; using default template."
    )
    return exp_args


def build_training_parameters_link(hub_model_id: Optional[str]) -> Optional[str]:
    if not hub_model_id:
        return None
    hub_model_id = hub_model_id.strip("/")
    return f"https://huggingface.co/{hub_model_id}/blob/main/config.json"


def _escape_template_braces(text: str) -> str:
    result: list[str] = []
    i = 0
    length = len(text)
    while i < length:
        char = text[i]
        if char == "$" and i + 1 < length and text[i + 1] == "{":
            j = i + 2
            while j < length and text[j] != "}":
                j += 1
            if j < length:
                result.append(text[i : j + 1])
                i = j + 1
                continue
        if char == "{":
            result.append("{{")
            i += 1
        elif char == "}":
            result.append("}}")
            i += 1
        else:
            result.append(char)
            i += 1
    return "".join(result)


def _normalize_strategy_value(value: Any) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, str):
        normalized = value.strip()
        if not normalized or normalized.lower() in {"none", "null", "false"}:
            return None
        return normalized
    return value


def _detect_distributed_strategy(exp_args: dict) -> Optional[str]:
    if _normalize_strategy_value(exp_args.get("deepspeed")):
        return "deepspeed"
    fsdp_cfg = exp_args.get("fsdp_config")
    if isinstance(fsdp_cfg, dict) and fsdp_cfg:
        return "fsdp"
    if _normalize_strategy_value(exp_args.get("fsdp")):
        return "fsdp"
    return None


def resolve_mixed_precision_setting(exp_args: dict) -> str:
    if _normalize_strategy_value(exp_args.get("fp8")):
        return "fp8"
    if exp_args.get("bf16") or exp_args.get("pure_bf16"):
        return "bf16"
    if exp_args.get("fp16"):
        return "fp16"
    return "no"


def _render_fsdp_config_block(exp_args: dict) -> str:
    fsdp_cfg = exp_args.get("fsdp_config")
    if isinstance(fsdp_cfg, dict) and fsdp_cfg:
        rendered = yaml.safe_dump(fsdp_cfg, sort_keys=False).strip()
    else:
        rendered = "\n".join(
            [
                "fsdp_version: 2",
                "fsdp_state_dict_type: SHARDED_STATE_DICT",
                "fsdp_offload_params: false",
                "fsdp_reshard_after_forward: true",
                "fsdp_cpu_ram_efficient_loading: true",
            ]
        )
    return textwrap.indent(rendered, "  ")


def build_accelerate_config_block(exp_args: dict) -> str:
    strategy = _detect_distributed_strategy(exp_args)
    if not strategy:
        return ""

    lines: list[str] = []
    accelerate_header = 'ACCELERATE_CONFIG_FILE="$TMP_DIR/${SLURM_JOB_ID}_accelerate_config.yaml.autogenerated"'
    if strategy == "deepspeed":
        ds_config_path = str(exp_args.get("deepspeed") or "").strip()
        lines.append(f"DEEPSPEED_CONFIG_FILE={ds_config_path}")
        lines.append(accelerate_header)
        lines.append("export ACCELERATE_CONFIG_FILE")
        lines.append('cat << EOT > "$ACCELERATE_CONFIG_FILE"')
        lines.append("# WARNING: auto-generated by launcher")
        lines.append("compute_environment: LOCAL_MACHINE")
        lines.append("deepspeed_config:")
        lines.append("  deepspeed_multinode_launcher: standard")
        lines.append("  deepspeed_config_file: $DEEPSPEED_CONFIG_FILE")
        lines.append("  zero3_init_flag: true")
        lines.append("distributed_type: DEEPSPEED")
        lines.append("fsdp_config:")
        lines.append("machine_rank: 0")
        lines.append("main_process_ip: $MASTER_ADDR")
        lines.append("main_process_port: $MASTER_PORT")
        lines.append("main_training_function: main")
        lines.append("num_machines: $SLURM_NNODES")
        lines.append("num_processes: $NUM_GPUS")
        lines.append("use_cpu: false")
        lines.append("EOT")
    elif strategy == "fsdp":
        mixed_precision = resolve_mixed_precision_setting(exp_args)
        fsdp_config_lines = _render_fsdp_config_block(exp_args)
        lines.append(accelerate_header)
        lines.append("export ACCELERATE_CONFIG_FILE")
        lines.append('cat << EOT > "$ACCELERATE_CONFIG_FILE"')
        lines.append("# WARNING: auto-generated by launcher")
        lines.append("compute_environment: LOCAL_MACHINE")
        lines.append("distributed_type: FSDP")
        lines.append(f"mixed_precision: {mixed_precision}")
        lines.append("fsdp_config:")
        if fsdp_config_lines:
            lines.extend(fsdp_config_lines.splitlines())
        lines.append("machine_rank: 0")
        lines.append("main_process_ip: $MASTER_ADDR")
        lines.append("main_process_port: $MASTER_PORT")
        lines.append("main_training_function: main")
        lines.append("num_machines: $SLURM_NNODES")
        lines.append("num_processes: $NUM_GPUS")
        lines.append("use_cpu: false")
        lines.append("EOT")

    block = "\n".join(lines).strip()
    if not block:
        return ""
    return _escape_template_braces(block) + "\n"
